{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# General\n",
    "import matplotlib\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "test_set = dsets.MNIST(root='./data', train=False, download=True)\n",
    "train_set = dsets.MNIST(root='./data', train=True, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data as numpy\n",
    "test_data = test_set.data.numpy()\n",
    "train_data = train_set.data.numpy()\n",
    "\n",
    "# Labels as numpy\n",
    "test_labels = test_set.targets.numpy()\n",
    "train_labels = train_set.targets.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7faec7c54828>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot sample data\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "img = test_data[0]\n",
    "plt.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test shape (flattened):  (10000, 784)\n",
      "Train shape (flattened):  (60000, 784)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 784)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Flatten train and test data\n",
    "test_flattened = test_data.reshape((test_data.shape[0], 784))\n",
    "train_flattened = train_data.reshape((train_data.shape[0], 784))\n",
    "\n",
    "print(\"Test shape (flattened): \", test_flattened.shape)\n",
    "print(\"Train shape (flattened): \", train_flattened.shape)\n",
    "\n",
    "np.array([train_flattened[0, :]]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test labels:  (10000, 10)\n",
      "Train labels:  (60000, 10)\n"
     ]
    }
   ],
   "source": [
    "# One hot encoding\n",
    "one_hot_test = np.eye(CLASSES)[test_labels]\n",
    "one_hot_train = np.eye(CLASSES)[train_labels]\n",
    "\n",
    "print(\"Test labels: \", one_hot_test.shape)\n",
    "print(\"Train labels: \", one_hot_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH = 20\n",
    "HIDDEN = 15\n",
    "DEBUG = False\n",
    "EPSILON = 0.005\n",
    "EPOCHS = 200 * 3000 # 5 passes over data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y, target,deriv = False):\n",
    "    if deriv:\n",
    "        return 2*(target - y)\n",
    "    return np.sum(np.square(target - y))\n",
    "\n",
    "# Sigmoid function for the\n",
    "# activation between input\n",
    "# and hidden layer, as well\n",
    "# as hidden to output layer\n",
    "# activation\n",
    "def sigmoid(x, deriv=False):\n",
    "    if(deriv == True):\n",
    "        return x*(1-x)\n",
    "    \n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# Softmax activation function\n",
    "# to normalize the output as\n",
    "# a vector of probabilities\n",
    "# for every single possible\n",
    "# class\n",
    "def softmax(x, deriv=False):\n",
    "    if(deriv == True):\n",
    "        return x*(1-x)\n",
    "\n",
    "    x = x - x.max(axis=1, keepdims=True)\n",
    "    y = np.exp(x)\n",
    "    sigma = y / y.sum(axis=1, keepdims=True)\n",
    "    return sigma\n",
    "\n",
    "# Feed forward operation\n",
    "def feed_forward(input_batch, w1, w2):\n",
    "    # Dot products\n",
    "    a1 = sigmoid(np.dot(input_batch,w1))\n",
    "    a1 = np.hstack((a1,np.ones((input_batch.shape[0],1))))\n",
    "    #print(a1.shape)\n",
    "    a2 = softmax(np.dot(a1,w2))\n",
    "    \n",
    "    return input_batch, a1, a2\n",
    "\n",
    "# Evaluate learnt weights\n",
    "def evaluate(test, w1, w2, labels):\n",
    "    # Metrics\n",
    "    corrects, wrongs = 0, 0\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    for i in range(len(test)):\n",
    "        # Add dimension\n",
    "        data = np.array([test[i]])\n",
    "        \n",
    "        # Run feedforward\n",
    "        _, _, res = feed_forward(data, w1, w2)\n",
    "        \n",
    "        # Check result\n",
    "        res_max = res.argmax()\n",
    "        if res_max == labels[i]:\n",
    "            corrects += 1\n",
    "        else:\n",
    "            wrongs += 1\n",
    "            \n",
    "    return corrects, wrongs\n",
    "\n",
    "def confusion_matrix(test_data, w1, w2, labels):\n",
    "        # Confusion matrix\n",
    "        cm = np.zeros((10, 10), int)\n",
    "        \n",
    "        for i in range(len(test_data)):\n",
    "            # Test sample\n",
    "            data = np.array([test_data[i]])\n",
    "            \n",
    "            # Get result\n",
    "            _, _, res = feed_forward(data, w1, w2)\n",
    "            res_max = res.argmax()\n",
    "            target = labels[i]\n",
    "            cm[res_max, int(target)] += 1\n",
    "            \n",
    "        return cm  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/youssefshoeb/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:15: RuntimeWarning: overflow encountered in exp\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  18.69965245733769\n",
      "MSE:  11.343890197230799\n",
      "MSE:  8.529086445932426\n",
      "MSE:  4.398482138513829\n",
      "MSE:  5.634308462447436\n",
      "MSE:  4.010166204105497\n",
      "MSE:  4.6966913289201475\n",
      "MSE:  4.395888141682924\n",
      "MSE:  4.431042057369202\n",
      "MSE:  3.1021032069360794\n",
      "MSE:  6.430239825952009\n",
      "MSE:  4.456503185210806\n",
      "MSE:  2.903455882967537\n",
      "MSE:  7.217057007965274\n",
      "MSE:  4.818375282470008\n",
      "MSE:  4.198406048278394\n",
      "MSE:  4.415005835413128\n",
      "MSE:  2.984428235695072\n",
      "MSE:  2.0789618114976736\n",
      "MSE:  3.2620134386523976\n",
      "MSE:  1.9579849755012517\n",
      "MSE:  2.5552374877556887\n",
      "MSE:  1.6910366442293085\n",
      "MSE:  4.128663024602384\n",
      "MSE:  1.318762979136134\n",
      "MSE:  1.373553423777511\n",
      "MSE:  3.3711582595416236\n",
      "MSE:  0.9377217382182492\n",
      "MSE:  3.81787134208969\n",
      "MSE:  3.3690004258410466\n",
      "MSE:  3.3522688107382\n",
      "MSE:  1.9429858533566717\n",
      "MSE:  2.4760246608899017\n",
      "MSE:  4.031341203232986\n",
      "MSE:  2.4284425202456354\n",
      "MSE:  3.912733569671735\n",
      "MSE:  3.2781428077342305\n",
      "MSE:  1.791197786700641\n",
      "MSE:  1.8012783105625627\n",
      "MSE:  4.575435175495996\n",
      "MSE:  1.5418849557775764\n",
      "MSE:  2.7818882787601558\n",
      "MSE:  2.7111373892830937\n",
      "MSE:  2.4756003867425505\n",
      "MSE:  0.9976243065139025\n",
      "MSE:  1.8643012617573314\n",
      "MSE:  1.9480924363753855\n",
      "MSE:  4.095707363578288\n",
      "MSE:  1.8182602226766371\n",
      "MSE:  2.5649337921249677\n",
      "MSE:  2.5286391769553442\n",
      "MSE:  1.4216482777633606\n",
      "MSE:  1.96438528554623\n",
      "MSE:  2.9553519784291344\n",
      "MSE:  1.3878403842748992\n",
      "MSE:  3.254367995615503\n",
      "MSE:  3.728767245492151\n",
      "MSE:  3.4699976056311095\n",
      "MSE:  2.5287119796616078\n",
      "MSE:  1.1976457673726193\n",
      "MSE:  1.2537215316323858\n",
      "MSE:  1.9974253930778774\n",
      "MSE:  1.9729501367521132\n",
      "MSE:  0.5845310964277983\n",
      "MSE:  1.683239092228936\n",
      "MSE:  1.400868044639309\n",
      "MSE:  1.7794639505396703\n",
      "MSE:  2.267761353311781\n",
      "MSE:  1.2482584514561863\n",
      "MSE:  1.1095720873909523\n",
      "MSE:  1.8657025127123683\n",
      "MSE:  2.702914279318189\n",
      "MSE:  1.9444104272874987\n",
      "MSE:  1.1960371744688003\n",
      "MSE:  0.9356446851455492\n",
      "MSE:  1.6436933701278327\n",
      "MSE:  2.2690907567727394\n",
      "MSE:  0.9707144646461533\n",
      "MSE:  1.1499363604425346\n",
      "MSE:  2.5769007363309817\n",
      "MSE:  2.198760383450794\n",
      "MSE:  1.9261893543392556\n",
      "MSE:  2.274339197691686\n",
      "MSE:  2.1417046946572587\n",
      "MSE:  1.1134087318363861\n",
      "MSE:  2.588658011228082\n",
      "MSE:  2.584481809819084\n",
      "MSE:  4.123954561325245\n",
      "MSE:  3.765063588722366\n",
      "MSE:  1.7985717957646856\n",
      "MSE:  2.1810110013373896\n",
      "MSE:  2.7230582457681782\n",
      "MSE:  2.0098182778199707\n",
      "MSE:  2.8582188360758245\n",
      "MSE:  1.9340246789433588\n",
      "MSE:  1.851461927613676\n",
      "MSE:  1.8852198869025005\n",
      "MSE:  1.10205549338432\n",
      "MSE:  1.2875316404917365\n",
      "MSE:  2.104734867020912\n",
      "MSE:  1.1077657752325414\n",
      "MSE:  0.5413210114314521\n",
      "MSE:  1.911001537031148\n",
      "MSE:  0.5531788552119068\n",
      "MSE:  0.3804449436566339\n",
      "MSE:  1.7036598080543093\n",
      "MSE:  1.4265752333099981\n",
      "MSE:  1.2089430463345547\n",
      "MSE:  1.328833138844737\n",
      "MSE:  0.9823705541043217\n",
      "MSE:  1.122880760318583\n",
      "MSE:  0.3571512989599021\n",
      "MSE:  0.4131814003178145\n",
      "MSE:  0.5832909991166781\n",
      "MSE:  1.0140612565587115\n",
      "MSE:  1.034196038457142\n",
      "MSE:  0.1467841498767005\n",
      "MSE:  0.2817741599779178\n",
      "MSE:  0.125670203528703\n",
      "MSE:  0.8785186038854853\n",
      "MSE:  0.8430209724212723\n",
      "MSE:  1.015253647349964\n",
      "MSE:  1.3857770731920123\n",
      "MSE:  0.4813275965212216\n",
      "MSE:  0.6509966760264323\n",
      "MSE:  1.5828348983168734\n",
      "MSE:  0.8121392538479232\n",
      "MSE:  0.8607248518693917\n",
      "MSE:  1.5439523283043772\n",
      "MSE:  1.2930066179398942\n",
      "MSE:  0.4199964133568665\n",
      "MSE:  1.2762664742866927\n",
      "MSE:  1.027347103800312\n",
      "MSE:  1.5129700954252605\n",
      "MSE:  0.5741946210250309\n",
      "MSE:  0.8434674157330102\n",
      "MSE:  0.9696117875874241\n",
      "MSE:  1.1408063984437988\n",
      "MSE:  0.4481916421502271\n",
      "MSE:  1.9568768765411875\n",
      "MSE:  2.2214292265897404\n",
      "MSE:  2.087288773760399\n",
      "MSE:  2.167958690912922\n",
      "MSE:  1.8989068380280698\n",
      "MSE:  2.110627687951632\n",
      "MSE:  0.9205820620322025\n",
      "MSE:  0.618171351253523\n",
      "MSE:  0.34009954999351916\n",
      "MSE:  0.30273461201135576\n",
      "MSE:  1.5060423202474575\n",
      "MSE:  1.0993109562443997\n",
      "MSE:  0.4741847538729026\n",
      "MSE:  2.325766857328496\n",
      "MSE:  1.371401518652689\n",
      "MSE:  1.6975804514984882\n",
      "MSE:  1.423812038937033\n",
      "MSE:  0.9151321215291671\n",
      "MSE:  1.7069143307684143\n",
      "MSE:  0.5828867807087513\n",
      "MSE:  0.7090111089149727\n",
      "MSE:  1.0109944918335474\n",
      "MSE:  0.4067391853958679\n",
      "MSE:  0.33898468242981183\n",
      "MSE:  1.082141177786844\n",
      "MSE:  2.1064713546179563\n",
      "MSE:  2.0949675228751405\n",
      "MSE:  0.32787522094070365\n",
      "MSE:  0.23820191050222533\n",
      "MSE:  1.2716919302312624\n",
      "MSE:  0.2560454015728664\n",
      "MSE:  0.31847948275656146\n",
      "MSE:  0.8747620223164354\n",
      "MSE:  1.3316161609166266\n",
      "MSE:  0.5950763894711959\n",
      "MSE:  0.3941064941484208\n",
      "MSE:  0.44964484834734997\n",
      "MSE:  0.3837940670911884\n",
      "MSE:  1.4525823190841927\n",
      "MSE:  0.1880458761782068\n",
      "MSE:  1.1305253171166512\n",
      "MSE:  0.1734495283351309\n",
      "MSE:  0.35245131116320916\n",
      "MSE:  0.5330067984082224\n",
      "MSE:  0.7001817853782716\n",
      "MSE:  2.1253585050348995\n",
      "MSE:  0.40624635912308354\n",
      "MSE:  0.5460677160043599\n",
      "MSE:  0.5376131569340257\n",
      "MSE:  0.5848234659180983\n",
      "MSE:  0.6966368118273469\n",
      "MSE:  0.6881378801130127\n",
      "MSE:  0.19072910284368397\n",
      "MSE:  0.8611936034603518\n",
      "MSE:  1.2013235082539695\n",
      "MSE:  0.6620326835638297\n",
      "MSE:  0.6670225064184435\n",
      "MSE:  0.6220315542839431\n",
      "MSE:  0.6550723649593819\n",
      "MSE:  0.21707090184922212\n",
      "MSE:  1.0838399866283033\n"
     ]
    }
   ],
   "source": [
    "# Fix random seed\n",
    "np.random.seed(1)\n",
    "\n",
    "# Weights\n",
    "w1 = 2 * np.random.random((784, HIDDEN)) - 1\n",
    "w2 = 2 * np.random.random((HIDDEN, 10)) - 1\n",
    "# Biases\n",
    "b0 = np.zeros((1,HIDDEN))\n",
    "b1 = np.zeros((1,10))\n",
    "\n",
    "\n",
    "# Biases with weights \n",
    "w1 = np.vstack((w1,b0))\n",
    "w2 = np.vstack((w2,b1))\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Index\n",
    "    idx = epoch % 3000\n",
    "    \n",
    "    # Feed forward process\n",
    "    input_batch = train_flattened[(idx * BATCH):((idx + 1) * BATCH), :]\n",
    "    input_batch = np.hstack((input_batch,np.ones((20,1))))\n",
    "    #print(input_batch.shape) \n",
    "    a0, a1, a2 = feed_forward(input_batch, w1, w2)\n",
    "    \n",
    "    if DEBUG:\n",
    "        print(\"A0 shape: \", a0.shape)\n",
    "        print(\"A1 shape: \", a1.shape)\n",
    "        print(\"A2 shape: \", a2.shape)\n",
    "    \n",
    "    # Compute mean error\n",
    "    y = one_hot_train[(idx * BATCH):((idx + 1) * BATCH), :]\n",
    "    \n",
    "    # Print MSE\n",
    "    if (epoch % 3000) == 0:\n",
    "        print(\"MSE: \", mse(a2,y))\n",
    "    \n",
    "    if DEBUG:\n",
    "        print(\"Error shape: \", error)\n",
    "        \n",
    "    # Compute DELTA2\n",
    "    delta2 = (a2 - y) * softmax(a2, True)\n",
    "    \n",
    "    # Compute dw2\n",
    "    dw2 = np.dot(a1.T, delta2)\n",
    "    \n",
    "    \n",
    "    # Compute DELTA1\n",
    "    delta1 = np.dot(delta2, w2.T) * sigmoid(a1, True)\n",
    "    \n",
    "    dw1 = np.dot(a0.T, delta1[:,0:-1])\n",
    "    #print(\"dw2 shape\",dw1.shape)\n",
    "    \n",
    "    \n",
    "    if DEBUG:\n",
    "        print(\"DELTA2 shape: \", delta2.shape)\n",
    "        print(\"dW2 shape: \", dw2.shape)\n",
    "        print(\"DELTA1 shape: \", delta1.shape)\n",
    "        print(\"dW1 shape: \", dw1.shape)\n",
    "        \n",
    "    \n",
    "    \n",
    "    # Update weights\n",
    "    w2 -= (EPSILON * dw2)\n",
    "    w1 -= (EPSILON * dw1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 784)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/youssefshoeb/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:15: RuntimeWarning: overflow encountered in exp\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "print(test_flattened.shape)\n",
    "test_flattened = np.hstack((test_flattened,np.ones((10000,1))))\n",
    "c, w = evaluate(test_flattened, w1, w2, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model accuracay is:  0.88\n"
     ]
    }
   ],
   "source": [
    "print(\"The model accuracay is: \", c / (c + w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/youssefshoeb/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:15: RuntimeWarning: overflow encountered in exp\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(test_flattened, w1, w2, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 919,    0,   22,    3,    1,   32,   31,    8,    5,   11],\n",
       "       [   0, 1103,    4,    2,    9,    3,    2,   16,    6,   10],\n",
       "       [   3,    2,  873,   45,    6,   10,   11,   29,    8,    4],\n",
       "       [   8,    8,   44,  806,    0,   27,    3,    6,   25,    4],\n",
       "       [   1,    0,   13,    2,  875,    7,   16,    8,   18,   30],\n",
       "       [  25,    6,    6,   85,    4,  739,   15,    2,   50,   25],\n",
       "       [  10,    1,   18,    3,   18,   11,  862,    0,   28,    5],\n",
       "       [   4,    3,   16,   17,    1,    8,    1,  925,   10,   11],\n",
       "       [   9,   12,   30,   35,    8,   44,   16,    9,  798,    9],\n",
       "       [   1,    0,    6,   12,   60,   11,    1,   25,   26,  900]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
