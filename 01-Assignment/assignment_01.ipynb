{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# General\n",
    "import matplotlib\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "test_set = dsets.MNIST(root='./data', train=False, download=True)\n",
    "train_set = dsets.MNIST(root='./data', train=True, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data as numpy\n",
    "test_data = test_set.data.numpy()\n",
    "train_data = train_set.data.numpy()\n",
    "\n",
    "# Labels as numpy\n",
    "test_labels = test_set.targets.numpy()\n",
    "train_labels = train_set.targets.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x12e96f518>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAM4ElEQVR4nO3db6xU9Z3H8c9nWZoY6QNQce9alC7xgc3GgCIxQTfXkDYsPsBGuikPGjZpvH2Apo0NWeM+wIeN2bZZn5DcRlO6YW1IqEqMcSHYSBq18WJQLr0BkbBwyxVsMCmYGES/++AeN1ecc2acMzNn4Pt+JZOZOd85Z74Z7odz5vyZnyNCAK5+f9N0AwAGg7ADSRB2IAnCDiRB2IEk/naQb2abXf9An0WEW02vtWa3vdb2EdvHbD9WZ1kA+svdHme3PU/SUUnfljQt6U1JGyPiTxXzsGYH+qwfa/ZVko5FxPGIuCjpt5LW11gegD6qE/abJJ2a83y6mPYFtsdsT9ieqPFeAGqqs4Ou1abClzbTI2Jc0rjEZjzQpDpr9mlJS+Y8/4ak0/XaAdAvdcL+pqRbbX/T9tckfV/S7t60BaDXut6Mj4hLth+W9D+S5kl6JiIO96wzAD3V9aG3rt6M7+xA3/XlpBoAVw7CDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBJdj88uSbZPSDov6VNJlyJiZS+aAtB7tcJeuC8i/tKD5QDoIzbjgSTqhj0k7bF9wPZYqxfYHrM9YXui5nsBqMER0f3M9t9HxGnbiyXtlfRIROyveH33bwagIxHhVtNrrdkj4nRxf1bSc5JW1VkegP7pOuy2r7X99c8fS/qOpMleNQagt+rsjb9R0nO2P1/Of0fEyz3pCkDP1frO/pXfjO/sQN/15Ts7gCsHYQeSIOxAEoQdSIKwA0n04kKYFDZs2FBae+ihhyrnPX36dGX9448/rqzv2LGjsv7++++X1o4dO1Y5L/JgzQ4kQdiBJAg7kARhB5Ig7EAShB1IgrADSXDVW4eOHz9eWlu6dOngGmnh/PnzpbXDhw8PsJPhMj09XVp78sknK+edmLhyf0WNq96A5Ag7kARhB5Ig7EAShB1IgrADSRB2IAmuZ+9Q1TXrt99+e+W8U1NTlfXbbrutsn7HHXdU1kdHR0trd999d+W8p06dqqwvWbKksl7HpUuXKusffPBBZX1kZKTr9z558mRl/Uo+zl6GNTuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJMH17FeBhQsXltaWL19eOe+BAwcq63fddVc3LXWk3e/lHz16tLLe7vyFRYsWldY2b95cOe+2bdsq68Os6+vZbT9j+6ztyTnTFtnea/vd4r78rw3AUOhkM/7XktZeNu0xSfsi4lZJ+4rnAIZY27BHxH5J5y6bvF7S9uLxdkkP9LYtAL3W7bnxN0bEjCRFxIztxWUvtD0maazL9wHQI32/ECYixiWNS+ygA5rU7aG3M7ZHJKm4P9u7lgD0Q7dh3y1pU/F4k6QXetMOgH5pe5zd9rOSRiVdL+mMpK2Snpe0U9LNkk5K+l5EXL4Tr9Wy2IxHxx588MHK+s6dOyvrk5OTpbX77ruvct5z59r+OQ+tsuPsbb+zR8TGktKaWh0BGChOlwWSIOxAEoQdSIKwA0kQdiAJLnFFYxYvLj3LWpJ06NChWvNv2LChtLZr167Kea9kDNkMJEfYgSQIO5AEYQeSIOxAEoQdSIKwA0kwZDMa0+7nnG+44YbK+ocfflhZP3LkyFfu6WrGmh1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkuB6dvTV6tWrS2uvvPJK5bzz58+vrI+OjlbW9+/fX1m/WnE9O5AcYQeSIOxAEoQdSIKwA0kQdiAJwg4kwfXs6Kt169aV1todR9+3b19l/fXXX++qp6zartltP2P7rO3JOdOesP1n2weLW/m/KICh0Mlm/K8lrW0x/ZcRsby4vdTbtgD0WtuwR8R+SecG0AuAPqqzg+5h2+8Um/kLy15ke8z2hO2JGu8FoKZuw75N0jJJyyXNSPp52QsjYjwiVkbEyi7fC0APdBX2iDgTEZ9GxGeSfiVpVW/bAtBrXYXd9sicp9+VNFn2WgDDoe1xdtvPShqVdL3taUlbJY3aXi4pJJ2Q9KP+tYhhds0111TW165tdSBn1sWLFyvn3bp1a2X9k08+qazji9qGPSI2tpj8dB96AdBHnC4LJEHYgSQIO5AEYQeSIOxAElziilq2bNlSWV+xYkVp7eWXX66c97XXXuuqJ7TGmh1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkmDIZlS6//77K+vPP/98Zf2jjz4qrVVd/ipJb7zxRmUdrTFkM5AcYQeSIOxAEoQdSIKwA0kQdiAJwg4kwfXsyV133XWV9aeeeqqyPm/evMr6Sy+Vj/nJcfTBYs0OJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0lwPftVrt1x8HbHuu+8887K+nvvvVdZr7pmvd286E7X17PbXmL797anbB+2/eNi+iLbe22/W9wv7HXTAHqnk834S5J+GhG3Sbpb0mbb35L0mKR9EXGrpH3FcwBDqm3YI2ImIt4qHp+XNCXpJknrJW0vXrZd0gN96hFAD3ylc+NtL5W0QtIfJd0YETPS7H8ItheXzDMmaaxmnwBq6jjsthdI2iXpJxHxV7vlPoAviYhxSePFMthBBzSko0NvtudrNug7IuJ3xeQztkeK+oiks/1pEUAvtF2ze3YV/rSkqYj4xZzSbkmbJP2suH+hLx2ilmXLllXW2x1aa+fRRx+trHN4bXh0shm/WtIPJB2yfbCY9rhmQ77T9g8lnZT0vb50CKAn2oY9Iv4gqewL+pretgOgXzhdFkiCsANJEHYgCcIOJEHYgST4KemrwC233FJa27NnT61lb9mypbL+4osv1lo+Boc1O5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kwXH2q8DYWPmvft188821lv3qq69W1gf5U+SohzU7kARhB5Ig7EAShB1IgrADSRB2IAnCDiTBcfYrwD333FNZf+SRRwbUCa5krNmBJAg7kARhB5Ig7EAShB1IgrADSRB2IIlOxmdfIuk3kv5O0meSxiPiP20/IekhSR8UL308Il7qV6OZ3XvvvZX1BQsWdL3sduOnX7hwoetlY7h0clLNJUk/jYi3bH9d0gHbe4vaLyPiP/rXHoBe6WR89hlJM8Xj87anJN3U78YA9NZX+s5ue6mkFZL+WEx62PY7tp+xvbBknjHbE7Yn6rUKoI6Ow257gaRdkn4SEX+VtE3SMknLNbvm/3mr+SJiPCJWRsTK+u0C6FZHYbc9X7NB3xERv5OkiDgTEZ9GxGeSfiVpVf/aBFBX27DbtqSnJU1FxC/mTB+Z87LvSprsfXsAeqWTvfGrJf1A0iHbB4tpj0vaaHu5pJB0QtKP+tAfanr77bcr62vWrKmsnzt3rpftoEGd7I3/gyS3KHFMHbiCcAYdkARhB5Ig7EAShB1IgrADSRB2IAkPcshd24zvC/RZRLQ6VM6aHciCsANJEHYgCcIOJEHYgSQIO5AEYQeSGPSQzX+R9L9znl9fTBtGw9rbsPYl0Vu3etnbLWWFgZ5U86U3tyeG9bfphrW3Ye1LorduDao3NuOBJAg7kETTYR9v+P2rDGtvw9qXRG/dGkhvjX5nBzA4Ta/ZAQwIYQeSaCTsttfaPmL7mO3HmuihjO0Ttg/ZPtj0+HTFGHpnbU/OmbbI9l7b7xb3LcfYa6i3J2z/ufjsDtpe11BvS2z/3vaU7cO2f1xMb/Szq+hrIJ/bwL+z254n6aikb0ualvSmpI0R8aeBNlLC9glJKyOi8RMwbP+TpAuSfhMR/1hMe1LSuYj4WfEf5cKI+Lch6e0JSReaHsa7GK1oZO4w45IekPSvavCzq+jrXzSAz62JNfsqScci4nhEXJT0W0nrG+hj6EXEfkmXD8myXtL24vF2zf6xDFxJb0MhImYi4q3i8XlJnw8z3uhnV9HXQDQR9psknZrzfFrDNd57SNpj+4DtsaabaeHGiJiRZv94JC1uuJ/LtR3Ge5AuG2Z8aD67boY/r6uJsLf6faxhOv63OiLukPTPkjYXm6voTEfDeA9Ki2HGh0K3w5/X1UTYpyUtmfP8G5JON9BHSxFxurg/K+k5Dd9Q1Gc+H0G3uD/bcD//b5iG8W41zLiG4LNrcvjzJsL+pqRbbX/T9tckfV/S7gb6+BLb1xY7TmT7Wknf0fANRb1b0qbi8SZJLzTYyxcMyzDeZcOMq+HPrvHhzyNi4DdJ6zS7R/49Sf/eRA8lff2DpLeL2+Gme5P0rGY36z7R7BbRDyVdJ2mfpHeL+0VD1Nt/STok6R3NBmukod7u0exXw3ckHSxu65r+7Cr6GsjnxumyQBKcQQckQdiBJAg7kARhB5Ig7EAShB1IgrADSfwfrLwRQB25h+kAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot sample data\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "img = test_data[0]\n",
    "plt.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test shape (flattened):  (10000, 784)\n",
      "Train shape (flattened):  (60000, 784)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 784)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Flatten train and test data\n",
    "test_flattened = test_data.reshape((test_data.shape[0], 784))\n",
    "train_flattened = train_data.reshape((train_data.shape[0], 784))\n",
    "\n",
    "print(\"Test shape (flattened): \", test_flattened.shape)\n",
    "print(\"Train shape (flattened): \", train_flattened.shape)\n",
    "\n",
    "np.array([train_flattened[0, :]]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test labels:  (10000, 10)\n",
      "Train labels:  (60000, 10)\n"
     ]
    }
   ],
   "source": [
    "# One hot encoding\n",
    "one_hot_test = np.eye(CLASSES)[test_labels]\n",
    "one_hot_train = np.eye(CLASSES)[train_labels]\n",
    "\n",
    "print(\"Test labels: \", one_hot_test.shape)\n",
    "print(\"Train labels: \", one_hot_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH = 20\n",
    "HIDDEN = 15\n",
    "DEBUG = False\n",
    "EPSILON = 0.005\n",
    "EPOCHS = 200 * 3000 # 5 passes over data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid function for the\n",
    "# activation between input\n",
    "# and hidden layer, as well\n",
    "# as hidden to output layer\n",
    "# activation\n",
    "def sigmoid(x, deriv=False):\n",
    "    if(deriv == True):\n",
    "        return x*(1-x)\n",
    "    \n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# Softmax activation function\n",
    "# to normalize the output as\n",
    "# a vector of probabilities\n",
    "# for every single possible\n",
    "# class\n",
    "def softmax(x, deriv=False):\n",
    "    if(deriv == True):\n",
    "        return x*(1-x)\n",
    "\n",
    "    x = x - x.max(axis=1, keepdims=True)\n",
    "    y = np.exp(x)\n",
    "    sigma = y / y.sum(axis=1, keepdims=True)\n",
    "    return sigma\n",
    "\n",
    "# Feed forward operation\n",
    "def feed_forward(input_batch, w1, w2):\n",
    "    # Dot products\n",
    "    a1 = sigmoid(np.dot(input_batch,w1))\n",
    "    a2 = softmax(np.dot(a1,w2))\n",
    "    \n",
    "    return input_batch, a1, a2\n",
    "\n",
    "# Evaluate learnt weights\n",
    "def evaluate(test, w1, w2, labels):\n",
    "    # Metrics\n",
    "    corrects, wrongs = 0, 0\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    for i in range(len(test)):\n",
    "        # Add dimension\n",
    "        data = np.array([test[i]])\n",
    "        \n",
    "        # Run feedforward\n",
    "        _, _, res = feed_forward(data, w1, w2)\n",
    "        \n",
    "        # Check result\n",
    "        res_max = res.argmax()\n",
    "        if res_max == labels[i]:\n",
    "            corrects += 1\n",
    "        else:\n",
    "            wrongs += 1\n",
    "            \n",
    "    return corrects, wrongs\n",
    "\n",
    "def confusion_matrix(test_data, w1, w2, labels):\n",
    "        # Confusion matrix\n",
    "        cm = np.zeros((10, 10), int)\n",
    "        \n",
    "        for i in range(len(test_data)):\n",
    "            # Test sample\n",
    "            data = np.array([test_data[i]])\n",
    "            \n",
    "            # Get result\n",
    "            _, _, res = feed_forward(data, w1, w2)\n",
    "            res_max = res.argmax()\n",
    "            target = labels[i]\n",
    "            cm[res_max, int(target)] += 1\n",
    "            \n",
    "        return cm  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dailand10/opt/miniconda3/envs/cvl/lib/python3.6/site-packages/ipykernel_launcher.py:10: RuntimeWarning: overflow encountered in exp\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  0.17007931527908882\n",
      "MSE:  0.12353610580655977\n",
      "MSE:  0.1119906943744497\n",
      "MSE:  0.09428694689075388\n",
      "MSE:  0.09825786445928379\n",
      "MSE:  0.07818498681729946\n",
      "MSE:  0.06772180104319186\n",
      "MSE:  0.0885970940120519\n",
      "MSE:  0.07827135201176776\n",
      "MSE:  0.08779400808436087\n",
      "MSE:  0.08505031119899652\n",
      "MSE:  0.09380642163277733\n",
      "MSE:  0.08848356533717615\n",
      "MSE:  0.09799443278349793\n",
      "MSE:  0.09213039391675917\n",
      "MSE:  0.07915860967702669\n",
      "MSE:  0.09917527949238565\n",
      "MSE:  0.09681059434603868\n",
      "MSE:  0.0907442786137883\n",
      "MSE:  0.09969421043431538\n",
      "MSE:  0.07764437747915692\n",
      "MSE:  0.08500499468042388\n",
      "MSE:  0.11281493840172875\n",
      "MSE:  0.09184315214046755\n",
      "MSE:  0.09091693850908418\n",
      "MSE:  0.08970867215786658\n",
      "MSE:  0.08260710701089527\n",
      "MSE:  0.10440887729817817\n",
      "MSE:  0.10162417020456037\n",
      "MSE:  0.07438243044491358\n",
      "MSE:  0.08785178409473109\n",
      "MSE:  0.07312345938600477\n",
      "MSE:  0.08010638174398188\n",
      "MSE:  0.08350152351937468\n",
      "MSE:  0.06751536658595596\n",
      "MSE:  0.07374108239849202\n",
      "MSE:  0.07637880724752633\n",
      "MSE:  0.07124838530368184\n",
      "MSE:  0.06994935517675005\n",
      "MSE:  0.08861315155421384\n",
      "MSE:  0.10050109419679791\n",
      "MSE:  0.10796151843887201\n",
      "MSE:  0.08956759523525765\n",
      "MSE:  0.08375682740806469\n",
      "MSE:  0.0666367474120841\n",
      "MSE:  0.06385289002509537\n",
      "MSE:  0.0597440671143631\n",
      "MSE:  0.0643545148458112\n",
      "MSE:  0.0739469842251143\n",
      "MSE:  0.07762076246601483\n",
      "MSE:  0.07227230911695016\n",
      "MSE:  0.07209864867408497\n",
      "MSE:  0.06995886799524254\n",
      "MSE:  0.05790304355379966\n",
      "MSE:  0.05659277303566669\n",
      "MSE:  0.06093601181788302\n",
      "MSE:  0.050401878766772296\n",
      "MSE:  0.0536125292527216\n",
      "MSE:  0.05284095967458971\n",
      "MSE:  0.04599231148140848\n",
      "MSE:  0.0411791997558616\n",
      "MSE:  0.044972109193413594\n",
      "MSE:  0.04409199364332325\n",
      "MSE:  0.04510945842547497\n",
      "MSE:  0.05570333296068065\n",
      "MSE:  0.06550447997386677\n",
      "MSE:  0.05924320898311953\n",
      "MSE:  0.05461913372036741\n",
      "MSE:  0.06166888698032924\n",
      "MSE:  0.0504232793346781\n",
      "MSE:  0.0537052668043526\n",
      "MSE:  0.0634739918697276\n",
      "MSE:  0.07810654804797093\n",
      "MSE:  0.08495994893648622\n",
      "MSE:  0.07696943340496455\n",
      "MSE:  0.06065545150413335\n",
      "MSE:  0.06016890669943459\n",
      "MSE:  0.05195563444180389\n",
      "MSE:  0.04782384641426262\n",
      "MSE:  0.04578624702446007\n",
      "MSE:  0.05173222720280119\n",
      "MSE:  0.04854688790823852\n",
      "MSE:  0.05637685199697945\n",
      "MSE:  0.049017224517873235\n",
      "MSE:  0.04762300469939969\n",
      "MSE:  0.04517730267876981\n",
      "MSE:  0.05129310251373186\n",
      "MSE:  0.04959792475960658\n",
      "MSE:  0.05301064705143707\n",
      "MSE:  0.053835935069618195\n",
      "MSE:  0.04707840166502168\n",
      "MSE:  0.06369225200697609\n",
      "MSE:  0.06280161212518266\n",
      "MSE:  0.05495193789449474\n",
      "MSE:  0.05665525965198725\n",
      "MSE:  0.04885759782151462\n",
      "MSE:  0.03548190297237466\n",
      "MSE:  0.05462018867553223\n",
      "MSE:  0.05306632833677641\n",
      "MSE:  0.0632223072690194\n",
      "MSE:  0.06450999363422508\n",
      "MSE:  0.06912612371730063\n",
      "MSE:  0.05184361366898429\n",
      "MSE:  0.05775569011392979\n",
      "MSE:  0.060982422932641854\n",
      "MSE:  0.07261638499435494\n",
      "MSE:  0.052223171430762425\n",
      "MSE:  0.054250143102158664\n",
      "MSE:  0.06765052277597293\n",
      "MSE:  0.07137619333091653\n",
      "MSE:  0.0599950341998256\n",
      "MSE:  0.05296871279994703\n",
      "MSE:  0.05994623584084154\n",
      "MSE:  0.07385924524844226\n",
      "MSE:  0.06612875413889766\n",
      "MSE:  0.055946702701723465\n",
      "MSE:  0.06319665496749366\n",
      "MSE:  0.05918543578838967\n",
      "MSE:  0.06180356914347349\n",
      "MSE:  0.05831649675668224\n",
      "MSE:  0.048469071146263\n",
      "MSE:  0.04918659304799345\n",
      "MSE:  0.048561309554386194\n",
      "MSE:  0.03882457665642878\n",
      "MSE:  0.04356189317798502\n",
      "MSE:  0.04411009270042473\n",
      "MSE:  0.033271140680285576\n",
      "MSE:  0.03654814175018151\n",
      "MSE:  0.04633921826948248\n",
      "MSE:  0.04381248573888221\n",
      "MSE:  0.04267728591024163\n",
      "MSE:  0.04438877163361466\n",
      "MSE:  0.05768603551629128\n",
      "MSE:  0.05797912092039642\n",
      "MSE:  0.0619914105246391\n",
      "MSE:  0.05974227982898124\n",
      "MSE:  0.05429907411595148\n",
      "MSE:  0.053420530548526406\n",
      "MSE:  0.049559296940976195\n",
      "MSE:  0.0494756380385104\n",
      "MSE:  0.042352041053806605\n",
      "MSE:  0.04251702548210858\n",
      "MSE:  0.04299108262391058\n",
      "MSE:  0.04344143747243555\n",
      "MSE:  0.04357256472578106\n",
      "MSE:  0.053827231538439484\n",
      "MSE:  0.050682321817000886\n",
      "MSE:  0.05503348678114456\n",
      "MSE:  0.049567599895722576\n",
      "MSE:  0.054878481876343914\n",
      "MSE:  0.037203209461436904\n",
      "MSE:  0.03805696986969982\n",
      "MSE:  0.036931845695077785\n",
      "MSE:  0.033437597216297264\n",
      "MSE:  0.034843737911774635\n",
      "MSE:  0.03511899124830714\n",
      "MSE:  0.04602642546122802\n",
      "MSE:  0.04183838118142519\n",
      "MSE:  0.038646706170626315\n",
      "MSE:  0.033079105004935085\n",
      "MSE:  0.040161745543271996\n",
      "MSE:  0.031335227349117555\n",
      "MSE:  0.04198937646377555\n",
      "MSE:  0.04082232020717253\n",
      "MSE:  0.054896734714037214\n",
      "MSE:  0.040940237042026976\n",
      "MSE:  0.03694727280059301\n",
      "MSE:  0.036055516068089016\n",
      "MSE:  0.04099970989962873\n",
      "MSE:  0.05407041104221392\n",
      "MSE:  0.05773000376937842\n",
      "MSE:  0.05187637809255943\n",
      "MSE:  0.0484396970705921\n",
      "MSE:  0.05183448487517838\n",
      "MSE:  0.04924451124839913\n",
      "MSE:  0.04744679444914134\n",
      "MSE:  0.04148511878351138\n",
      "MSE:  0.043774732506594825\n",
      "MSE:  0.044388636219581515\n",
      "MSE:  0.058317517899338187\n",
      "MSE:  0.04723075075125436\n",
      "MSE:  0.049639774637416256\n",
      "MSE:  0.034013327018086736\n",
      "MSE:  0.04867441089383279\n",
      "MSE:  0.062165973155540614\n",
      "MSE:  0.06187456401557271\n",
      "MSE:  0.05250831480855048\n",
      "MSE:  0.04970463459004977\n",
      "MSE:  0.05069513379877254\n",
      "MSE:  0.07290796840814752\n",
      "MSE:  0.053907614101936424\n",
      "MSE:  0.0488428939125162\n",
      "MSE:  0.05482996537690142\n",
      "MSE:  0.055972228945468515\n",
      "MSE:  0.05561127302104461\n",
      "MSE:  0.06335563674999554\n",
      "MSE:  0.06111333957299654\n",
      "MSE:  0.053279903267242815\n",
      "MSE:  0.053839107429946464\n",
      "MSE:  0.049798847190495696\n"
     ]
    }
   ],
   "source": [
    "# Fix random seed\n",
    "np.random.seed(1)\n",
    "\n",
    "# Weights\n",
    "w1 = 2 * np.random.random((784, HIDDEN)) - 1\n",
    "w2 = 2 * np.random.random((HIDDEN, 10)) - 1\n",
    "\n",
    "# Biases\n",
    "# b0 = 2 * np.random.random((784, HIDDEN)) - 1\n",
    "# b1 = 2 * np.random.random((HIDDEN, 10)) - 1\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Index\n",
    "    idx = epoch % 3000\n",
    "    \n",
    "    # Feed forward process\n",
    "    input_batch = train_flattened[(idx * BATCH):((idx + 1) * BATCH), :]\n",
    "    a0, a1, a2 = feed_forward(input_batch, w1, w2)\n",
    "    \n",
    "    if DEBUG:\n",
    "        print(\"A0 shape: \", a0.shape)\n",
    "        print(\"A1 shape: \", a1.shape)\n",
    "        print(\"A2 shape: \", a2.shape)\n",
    "    \n",
    "    # Compute mean error\n",
    "    y = one_hot_train[(idx * BATCH):((idx + 1) * BATCH), :]\n",
    "    \n",
    "    if DEBUG:\n",
    "        print(\"Error shape: \", error)\n",
    "        \n",
    "    # Compute DELTA2\n",
    "    delta2 = (a2 - y)\n",
    "    \n",
    "    # Compute dw2\n",
    "    dw2 = np.dot(a1.T, delta2)\n",
    "    \n",
    "    # Compute db2\n",
    "    \n",
    "    # Compute DELTA1\n",
    "    delta1 = np.dot(delta2, w2.T) * sigmoid(a1, True)\n",
    "    \n",
    "    # Compute dw1\n",
    "    dw1 = np.dot(delta1.T, a0).T\n",
    "    \n",
    "    #TODO: Compute db1\n",
    "    \n",
    "    if DEBUG:\n",
    "        print(\"DELTA2 shape: \", delta2.shape)\n",
    "        print(\"dW2 shape: \", dw2.shape)\n",
    "        print(\"DELTA1 shape: \", delta1.shape)\n",
    "        print(\"dW1 shape: \", dw1.shape)\n",
    "        \n",
    "    # Print MSE\n",
    "    if (epoch % 3000) == 0:\n",
    "        print(\"MSE: \", np.mean(np.abs(delta2)))\n",
    "    \n",
    "    # Update weights\n",
    "    w2 -= (EPSILON * dw2)\n",
    "    w1 -= (EPSILON * dw1)\n",
    "    \n",
    "    #TODO: Update basis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dailand10/opt/miniconda3/envs/cvl/lib/python3.6/site-packages/ipykernel_launcher.py:10: RuntimeWarning: overflow encountered in exp\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "c, w = evaluate(test_flattened, w1, w2, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model accuracay is:  0.803\n"
     ]
    }
   ],
   "source": [
    "print(\"The model accuracay is: \", c / (c + w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dailand10/opt/miniconda3/envs/cvl/lib/python3.6/site-packages/ipykernel_launcher.py:10: RuntimeWarning: overflow encountered in exp\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(test_flattened, w1, w2, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 903,    0,   13,   34,    4,   28,   30,   11,    6,    5],\n",
       "       [   0, 1083,   27,   20,    3,    7,   12,   84,   25,   13],\n",
       "       [  22,    2,  748,   20,    4,   10,   19,   21,    3,    4],\n",
       "       [   3,   10,   21,  832,    0,  263,    5,   14,  126,   12],\n",
       "       [   1,    2,   11,    4,  871,    3,   20,   15,   20,  111],\n",
       "       [  24,    5,   41,   22,    0,  518,   62,    8,  107,   16],\n",
       "       [  18,    6,   53,    0,   26,   18,  808,    0,   15,    6],\n",
       "       [   6,    0,   18,   10,    4,    7,    1,  830,    3,   38],\n",
       "       [   2,   27,   93,   62,   14,   32,    1,    3,  656,   23],\n",
       "       [   1,    0,    7,    6,   56,    6,    0,   42,   13,  781]])"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
