{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Administrative\n",
    "- Last meeting in 2020 will be on 18th.\n",
    "- First meeting in 2021 will be on 15th.\n",
    "- Colab tips:\n",
    "    https://medium.com/@oribarel/getting-the-most-out-of-your-google-colab-2b0585f82403\n",
    "    https://medium.com/datadriveninvestor/speed-up-your-image-training-on-google-colab-dc95ea1491cf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizers\n",
    "<img src=opt.jpg>\n",
    "<img src=opt1.jpg>\n",
    "<img src=opt2.jpg>\n",
    "<img src=opt3.jpg>\n",
    "<img src=opt4.jpg>\n",
    "<img src=opt5.jpg>\n",
    "<img src=opt6.jpg>\n",
    "<img src=opt7.jpg>\n",
    "<img src=opt8.jpg>\n",
    "<img src=opt9.jpg>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Curves\n",
    "<img src=curve.jpg>\n",
    "<img src=curve1.jpg>\n",
    "<img src=curve11.jpg>\n",
    "<img src=curve3.jpg>\n",
    "<img src=curve4.jpg>\n",
    "<img src=curve5.jpg>\n",
    "<img src=curve6.jpg>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why deep learning works?\n",
    "<img src=A0.png>\n",
    "<img src=A1.png>\n",
    "\n",
    "### Establish a single-number evaluation metric for you to optimize\n",
    "\n",
    "<img src=A2.png>\n",
    "<img src=A3.png>\n",
    "First, define what an “acceptable” running time is. Let's say anything that runs in 100ms is acceptable. Then, maximize accuracy, subject to your classifier meeting the running time criteria.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Is it good to have end-to-end in these scenarios?\n",
    "- Image captioning\n",
    "- Predicting the age of a child using XRay?\n",
    "- Self-Driving?\n",
    "\n",
    "## Is it a good idea to split your dataset into train and test sets?\n",
    "\n",
    "## Why is it a good idea to know human performance?\n",
    "\n",
    "Train: 8%\n",
    "Dev:   10%\n",
    "\n",
    "What should I do?\n",
    "\n",
    "Now, what is I tell you human error is 1%? or 7.5%?\n",
    "\n",
    "\n",
    "------------\n",
    "Now we know human error is important. Which one do you prefer in case of medical learning problems?\n",
    "\n",
    "Typical human 3%\n",
    "Typical Doctor 1%\n",
    "Expert Doctor 0.7%\n",
    "Board of Experts 0.5%\n",
    "\n",
    "<img src=A7.png>\n",
    "\n",
    "\n",
    "\n",
    "## In most deep learning problems, train and test come from different distributions\n",
    "\n",
    "For example, suppose you are working on implementing a vision for a soccer robot and have gathered two chunks of data: the first, larger chunk comes from many youtube videos and some from your lab, and the second, much smaller chunk is your actual robot playing soccer on the field with other robots.\n",
    "\n",
    "In this case, splitting the data into train/dev/test can be tricky.\n",
    "\n",
    "<img src=A4.png>\n",
    "<img src=A5.png>\n",
    "<img src=A6.png>\n",
    "\n",
    "\n",
    "## don't forget data synthesis!\n",
    "\n",
    "Assume you want to design a self-driving car. Is it a good idea to train on snapshots of game scenes like GTA?\n",
    "\n",
    "If you are interested, continue reading about it on \"machine learning yearning\" from Andrew NG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Networks (ConvNet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Watch : http://setosa.io/ev/image-kernels/\n",
    "# Convolution Operation\n",
    "<img src=Kernel.jpg>\n",
    "<img src=s.jpg>\n",
    "<img src=s1.jpg>\n",
    "<img src=giphy.gif>\n",
    "<img src=s2.jpg>\n",
    "<img src=s3.jpg>\n",
    "<img src=s4.jpg>\n",
    "<img src=s5.jpg>\n",
    "<img src=s6.jpg>\n",
    "<img src=s7.jpg>\n",
    "<img src=s8.jpg>\n",
    "<img src=s9.jpg>\n",
    "<img src=s10.jpg>\n",
    "<img src=s11.jpg>\n",
    "<img src=s12.jpg>\n",
    "<img src=s13.jpg>\n",
    "<img src=s14.jpg>\n",
    "<img src=s15.jpg>\n",
    "<img src=s16.jpg>\n",
    "<img src=s17.jpg>\n",
    "<img src=s18.jpg>\n",
    "<img src=s19.jpg>\n",
    "<img src=s20.jpg>\n",
    "<img src=s21.jpg>\n",
    "<img src=s22.jpg>\n",
    "<img src=s23.jpg>\n",
    "<img src=s24.jpg>\n",
    "<img src=s25.jpg>\n",
    "<img src=s26.jpg>\n",
    "<img src=s27.jpg>\n",
    "<img src=s28.jpg>\n",
    "<img src=s29.jpg>\n",
    "<img src=s30.jpg>\n",
    "<img src=s31.jpg>\n",
    "<img src=s32.jpg>\n",
    "Weight sharing\n",
    "Receptive field\n",
    "\n",
    "# Pooling\n",
    "<img src=pooling1.png>\n",
    "<img src=pooling.png>\n",
    "# CNN\n",
    "<img src=cnn.png>\n",
    "<img src=cnn2.png>\n",
    "<img src=car.png>\n",
    "<img src=filt1.jpeg>\n",
    "<img src=vis.jpg>\n",
    "http://cs231n.github.io/convolutional-networks/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://scs.ryerson.ca/~aharley/vis/conv/flat.html\n",
    "http://scs.ryerson.ca/~aharley/vis/conv/\n",
    "<img src=mnist.jpg>\n",
    "https://www.youtube.com/watch?v=IePwfYXZok0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some technical terms\n",
    "- Kernel / (Convolutional)Filter\n",
    "- Feature map / Activation map\n",
    "- Receptive field\n",
    "- Filter bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 500. Loss: 0.3889637887477875. Accuracy: 87.5\n",
      "Iteration: 1000. Loss: 0.19490301609039307. Accuracy: 93.12999725341797\n",
      "Iteration: 1500. Loss: 0.36571189761161804. Accuracy: 94.66000366210938\n",
      "Iteration: 2000. Loss: 0.1817563772201538. Accuracy: 95.27999877929688\n",
      "Iteration: 2500. Loss: 0.10491865128278732. Accuracy: 96.12999725341797\n",
      "Iteration: 3000. Loss: 0.06813360750675201. Accuracy: 96.6500015258789\n",
      "Iteration: 3500. Loss: 0.12628273665905. Accuracy: 96.97000122070312\n",
      "Iteration: 4000. Loss: 0.11794233322143555. Accuracy: 97.08999633789062\n",
      "Iteration: 4500. Loss: 0.2517227828502655. Accuracy: 97.44999694824219\n",
      "Iteration: 5000. Loss: 0.14075274765491486. Accuracy: 97.5199966430664\n",
      "Iteration: 5500. Loss: 0.1262623369693756. Accuracy: 97.61000061035156\n",
      "Iteration: 6000. Loss: 0.06749948114156723. Accuracy: 97.76000213623047\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    " \n",
    "'''\n",
    "STEP 1: LOADING DATASET\n",
    "'''\n",
    " \n",
    "train_dataset = dsets.MNIST(root='./data', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    " \n",
    "test_dataset = dsets.MNIST(root='./data', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())\n",
    " \n",
    "'''\n",
    "STEP 2: MAKING DATASET ITERABLE\n",
    "'''\n",
    " \n",
    "batch_size = 100\n",
    "n_iters = 6000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    " \n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    " \n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    " \n",
    "'''\n",
    "STEP 3: CREATE MODEL CLASS\n",
    "'''\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "         \n",
    "        # Convolution 1\n",
    "        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=0)\n",
    "        self.relu1 = nn.ReLU()\n",
    "         \n",
    "        # Max pool 1\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n",
    "      \n",
    "        # Convolution 2\n",
    "        self.cnn2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=0)\n",
    "        self.relu2 = nn.ReLU()\n",
    "         \n",
    "        # Max pool 2\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n",
    "         \n",
    "        # Fully connected 1 (readout)\n",
    "        self.fc1 = nn.Linear(32 * 4 * 4, 10) \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x =  torch.Size([100, 1, 28, 28])\n",
    "        # Convolution 1\n",
    "        out = self.cnn1(x) #torch.Size([100, 16, 24, 24])\n",
    "        out = self.relu1(out) #torch.Size([100, 16, 24, 24])\n",
    "         \n",
    "        # Max pool 1\n",
    "        out = self.maxpool1(out) #torch.Size([100, 16, 12, 12])\n",
    "         \n",
    "        # Convolution 2 \n",
    "        out = self.cnn2(out) #torch.Size([100, 32, 8, 8])\n",
    "        out = self.relu2(out) #torch.Size([100, 32, 8, 8])\n",
    "         \n",
    "        # Max pool 2 \n",
    "        out = self.maxpool2(out) #torch.Size([100, 32, 4, 4])\n",
    "         \n",
    "\n",
    "        out = out.view(out.size(0), -1)   #torch.Size([100, 512])\n",
    " \n",
    "        # Linear function (readout)\n",
    "        out = self.fc1(out)\n",
    "         \n",
    "        return out\n",
    " \n",
    "'''\n",
    "STEP 4: INSTANTIATE MODEL CLASS\n",
    "'''\n",
    " \n",
    "model = CNNModel()\n",
    " \n",
    "#######################\n",
    "#  USE GPU FOR MODEL  #\n",
    "#######################\n",
    " \n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    " \n",
    "'''\n",
    "STEP 5: INSTANTIATE LOSS CLASS\n",
    "'''\n",
    "criterion = nn.CrossEntropyLoss()\n",
    " \n",
    " \n",
    "'''\n",
    "STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
    "'''\n",
    "learning_rate = 0.01\n",
    " \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    " \n",
    "'''\n",
    "STEP 7: TRAIN THE MODEL\n",
    "'''\n",
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "         \n",
    "        #######################\n",
    "        #  USE GPU FOR MODEL  #\n",
    "        #######################\n",
    "        if torch.cuda.is_available():\n",
    "            images = images.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "         \n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "         \n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(images)\n",
    "         \n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "         \n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "         \n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "         \n",
    "        iter += 1\n",
    "         \n",
    "        if iter % 500 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "                #######################\n",
    "                #  USE GPU FOR MODEL  #\n",
    "                #######################\n",
    "                if torch.cuda.is_available():\n",
    "                    images = images.cuda()\n",
    "                 \n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "                 \n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                 \n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "                 \n",
    "                # Total correct predictions\n",
    "                correct += (predicted.cpu() == labels.cpu()).sum().float()\n",
    "\n",
    "             \n",
    "            accuracy = 100. * correct / total\n",
    "             \n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment4\n",
    "- Train a CNN for CIFAR-10 with the best hyperparameters possible\n",
    "- Visualize activations after each Conv filter\n",
    "- Visualize kernel of each Conv filter\n",
    "- Which one is working better in CIFAR-10, MaxPooling or AvgPooling or concatination of both\n",
    "- Take a Convolutional layer with the appropriate kernel size and load the trained weight/bias into an FC layer (You should think about in what situation you can have convolutional layer like FC layer)\n",
    "- Extra Point:\n",
    "    Add a self-attention layer to your CIFAR-10 model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refrences:\n",
    "### CS231n Standford\n",
    "### https://www.udemy.com/course/practical-deep-learning-with-pytorch/\n",
    "### https://www.deeplearning.ai/machine-learning-yearning/\n",
    "### \"Nuts and Bolts of Applying Deep Learning (Andrew Ng)\": https://www.youtube.com/watch?v=F1ka6a13S9I\n",
    "### https://kevinzakka.github.io/2016/09/26/applying-deep-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
