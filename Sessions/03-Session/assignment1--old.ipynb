{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nFGeuQt56zQp"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PyeZsSMQ6zQt"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9yAkHH9Ancip"
   },
   "outputs": [],
   "source": [
    "# http://pytorch.org/\n",
    "from os.path import exists\n",
    "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
    "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
    "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
    "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
    "\n",
    "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f69XfXpG6zQw"
   },
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self):\n",
    "        self.w = np.random.normal(size=(785, 10))\n",
    "        self.w[0, :] = 0\n",
    "        self.lr = 0.01\n",
    "        self.batch_size = 20\n",
    "\n",
    "\n",
    "def softmax(self, x):\n",
    "    x = x - np.max(x, axis=1).reshape((-1,1))\n",
    "    x = np.exp(x)\n",
    "    return x / np.sum(x, axis=1).reshape((-1, 1))\n",
    "\n",
    "def softmax_der(self, softmax):\n",
    "    '''\n",
    "    If you dont know how to calc it see:\n",
    "    https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/\n",
    "    https://math.stackexchange.com/questions/945871/derivative-of-softmax-loss-function\n",
    "    https://deepnotes.io/softmax-crossentropy\n",
    "    https://medium.com/@aerinykim/how-to-implement-the-softmax-derivative-independently-from-any-loss-function-ae6d44363a9d\n",
    "    https://stackoverflow.com/questions/40575841/numpy-calculate-the-derivative-of-the-softmax-function\n",
    "    '''\n",
    "    result = np.zeros((softmax.shape[0], softmax.shape[1], softmax.shape[1]))\n",
    "    for idx in range(softmax.shape[0]):\n",
    "        result_tmp = (np.diagflat(softmax[idx]) -\n",
    "                      np.dot(softmax[idx].reshape((-1, 1)),\n",
    "                             softmax[idx].reshape((1, -1))))\n",
    "        result[idx] = result_tmp\n",
    "\n",
    "    return result\n",
    "\n",
    "def softmax_der_vectorized(self, softmax):\n",
    "    '''\n",
    "    http://ajcr.net/Basic-guide-to-einsum/\n",
    "    https://stackoverflow.com/questions/48627163/construct-n1-dimensional-diagonal-matrix-from-values-in-n-dimensional-array\n",
    "    '''\n",
    "    diagnal = np.zeros((*softmax.shape, softmax.shape[-1]), softmax.dtype)\n",
    "    np.einsum('...jj->...j', diagnal)[...] = softmax\n",
    "    return diagnal-np.einsum('ij,i...->ij...',softmax,softmax)\n",
    "    \n",
    "    def mse_loss(self, predicted, target):\n",
    "        return np.sum(np.square(target - predicted))\n",
    "\n",
    "    def mse_der(self, predicted, target):\n",
    "        result = 2*(predicted - target)\n",
    "        return result[..., np.newaxis]\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        input_data = np.hstack((np.ones(self.batch_size).reshape(-1, 1), input_data))\n",
    "        self._data = input_data\n",
    "        return self.softmax(np.inner(input_data, self.w.T))\n",
    "\n",
    "    def backward(self, predicted, target):\n",
    "        delta = self.mse_der(predicted, target) *  self.softmax_der_vectorized(predicted)\n",
    "        delta = np.sum(delta, axis=1)\n",
    "   \n",
    "        update = np.dot(self._data.T, delta)\n",
    "        \n",
    "        self.w -= self.lr * update\n",
    "        return self.mse_loss(predicted, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PVFeJ6tr6zQy"
   },
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "train_dataset= dsets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset= dsets.MNIST(root='./data', train=False, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 738
    },
    "colab_type": "code",
    "id": "gTjQbiZ26zQ0",
    "outputId": "1f5df299-739b-4929-9bac-4c2ceb501ea7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 10 epochs\n",
      "Epoch 0\n",
      "[0/3000]Loss:  33.609620349549225\n",
      "[1000/3000]Loss:  21.81188541909328\n",
      "[2000/3000]Loss:  14.472095336450208\n",
      "Epoch 1\n",
      "[0/3000]Loss:  10.181574589920944\n",
      "[1000/3000]Loss:  11.620620287393734\n",
      "[2000/3000]Loss:  8.177264129520966\n",
      "Epoch 2\n",
      "[0/3000]Loss:  9.156730459009804\n",
      "[1000/3000]Loss:  7.998133788065669\n",
      "[2000/3000]Loss:  10.891740814890941\n",
      "Epoch 3\n",
      "[0/3000]Loss:  5.557405341184383\n",
      "[1000/3000]Loss:  12.51427489952469\n",
      "[2000/3000]Loss:  3.98951496719494\n",
      "Epoch 4\n",
      "[0/3000]Loss:  3.4146170312996316\n",
      "[1000/3000]Loss:  12.789549485792115\n",
      "[2000/3000]Loss:  4.205618289211201\n",
      "Epoch 5\n",
      "[0/3000]Loss:  3.885511574926349\n",
      "[1000/3000]Loss:  4.775450359579029\n",
      "[2000/3000]Loss:  7.278697003932824\n",
      "Epoch 6\n",
      "[0/3000]Loss:  5.1018734808151915\n",
      "[1000/3000]Loss:  1.5425730747371793\n",
      "[2000/3000]Loss:  0.10309253480573724\n",
      "Epoch 7\n",
      "[0/3000]Loss:  4.79644239681873\n",
      "[1000/3000]Loss:  4.736821875760951\n",
      "[2000/3000]Loss:  5.315254708540284\n",
      "Epoch 8\n",
      "[0/3000]Loss:  4.265360212973095\n",
      "[1000/3000]Loss:  3.0866148734833394\n",
      "[2000/3000]Loss:  3.6136697715821398\n",
      "Epoch 9\n",
      "[0/3000]Loss:  2.6563559404284987\n",
      "[1000/3000]Loss:  0.6271809718748271\n",
      "[2000/3000]Loss:  1.5837091955315403\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "mlp = MLP()\n",
    "print('Train for %d epochs' % epochs)\n",
    "for epoch in range(epochs):\n",
    "    print('Epoch %d' % epoch)\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data = data.numpy().squeeze().reshape((batch_size, -1))\n",
    "        target = target.numpy()\n",
    "        one_hot = np.array([np.eye(10)[i] for i in target])\n",
    "        output = mlp.forward(data)\n",
    "        loss = mlp.backward(output, one_hot)\n",
    "        if batch_idx % 1000 == 0:\n",
    "            print('[%d/%d]Loss: ' % (batch_idx, len(train_loader)), loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "3nbBZuIY6zQ5",
    "outputId": "9ab5ef4d-e5a9-4078-e310-258316c80133"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test\n",
      "Test accuracy:  0.9101\n"
     ]
    }
   ],
   "source": [
    "print('Test')\n",
    "correct_labels = 0\n",
    "for batch_idx, (data, target) in enumerate(test_loader):\n",
    "    data = data.numpy().squeeze().reshape((batch_size, -1))\n",
    "    target = target.numpy()\n",
    "    output = mlp.forward(data)\n",
    "    predicted = np.argmax(output, axis=1)\n",
    "    correct_labels += np.sum(predicted == target)\n",
    "\n",
    "print('Test accuracy: ', correct_labels / len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rHKUvIYo6zQ8"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "assignment1_kukleva.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
